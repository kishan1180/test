import boto3
from configparser import ConfigParser
import psycopg2
from datetime import datetime
import tempfile
import json
import logging

# Function to download the config file from S3
def download_config_from_s3(bucket_name, key, local_filename):
    s3 = boto3.client('s3')
    s3.download_file(bucket_name, key, local_filename)

# Function to parse the config file with delimiters
def parse_config_file_with_delimiter(config_file_object):
    config_lines = config_file_object.get()['Body'].read().decode('utf-8').split('\n')

    # Assuming the first line contains the header and delimiter is "|"
    header_line = config_lines[0]
    delimiter = "|"

    # Extracting values using the delimiter
    config_values = header_line.split(delimiter)

    # Assuming src name is the first value
    src_name = config_values[0].strip()

    return src_name

# Function to parse the config file
def parse_config_file(config_filename):
    config = ConfigParser()
    config.read(config_filename)
    return config

# Creating SQL based on conditions
def update_sql(source_name, start_time, end_time, temp_file_path):
    # Connect to your PostgreSQL database
    # Replace these values with your actual database connection details
    connection = psycopg2.connect(
        host='your_host',
        database='your_database',
        user='your_user',
        password='your_password'
    )

    cursor = connection.cursor()

    # Condition for source_name
    if source_name == 'EDI_TO_LAI_Monthly':
        sql_query = "UPDATE EDI_to_LAI_monthly SET start_time = %s, end_time = %s;"
        sql_file_name = 'config/app/sql/EDI_To_LAI_Monthly_Vending_Recon_TMP.sql'
    elif source_name == 'LBE':
        sql_query = "UPDATE LBE SET start_time = %s, end_time = %s;"
        sql_file_name = 'config/app/sql/Best_Execution_File_and_Vending_Loan_TMP.sql'
    elif source_name == 'PFB':
        sql_query = "UPDATE PFB SET start_time = %s, end_time = %s;"
        sql_file_name = 'config/app/sql/PFP_File_and_Vending_Loan_Count_TMP.sql'
    else:
        print(f"Unsupported source name: {source_name}")
        return

    # Execute the SQL query
    cursor.execute(sql_query, (start_time, end_time))
    connection.commit()

    # Save the changes to a temporary file
    with open(temp_file_path, 'w') as temp_file:
        temp_file.write(f"{source_name} SQL updated with start time: {start_time}, end time: {end_time}")

    # Update SQL file and upload to new location
    with open(sql_file_name, 'r') as sql_file:
        sql_content = sql_file.read()

    # Modify the SQL content with new times

    new_sql_file_name = f"prepare/cin/dflt/new_sql_loc/{sql_file_name.split('/')[-1]}"
    with open(new_sql_file_name, 'w') as new_sql_file:
        new_sql_file.write(sql_content)

    cursor.close()
    connection.close()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Lambda function to invoke
def lambda_invoke(source_name, OracleExtractARN):
    lambda_client = boto3.client('lambda')
    logger.info(f"triggering lambda with arn {OracleExtractARN}")
    config_data = {'srcName': source_name}
    response = lambda_client.invoke(FunctionName=OracleExtractARN,
                                    InvocationType='Event',
                                    Payload=json.dumps(config_data))

    return response.get('Payload').read()

def main():

    # s3_bucket_name = 'your_s3_bucket'
    # file_keys = [
    #     'inbox/cin/default/file_uploads/EDI_TO_LAI_Monthly_Upload1.YYYYMMDD.dat',
    #     'inbox/cin/default/file_uploads/EDI_TO_LAI_Monthly_Upload2.YYYYMMDD.dat',
    #     # Add more file keys as needed
    # ]

    # for s3_key in file_keys:


    s3_bucket_name = 'your_s3_bucket'
    s3_key = 'inbox/cin/default/file_uploads/EDI_TO_LAI_Monthly_Upload.YYYYMMDD.dat'
    
    local_config_filename = 'config.ini'

    # Temporary file to store changes
    temp_file_path = tempfile.mktemp(suffix='.txt')

    download_config_from_s3(s3_bucket_name, s3_key, local_config_filename)

    # Assuming s3_client is already defined
    s3_client = boto3.resource("s3")
    s3_ConfigFileObject = s3_client.Object(s3_bucket_name, s3_key)
    source_name = parse_config_file_with_delimiter(s3_ConfigFileObject)

    config = parse_config_file(local_config_filename)

    # Extracting values
    start_time_str = config.get('Settings', 'start_time')
    end_time_str = config.get('Settings', 'end_time')

    # Convert start_time and end_time to

    start_time = datetime.strptime(start_time_str, '%Y-%m-%d %H:%M:%S')
    end_time = datetime.strptime(end_time_str, '%Y-%m-%d %H:%M:%S')

    update_sql(source_name, start_time, end_time, temp_file_path)
    print(f"Changes saved to temporary file: {temp_file_path}")

    # Assuming lambdaClient is already defined
    OracleExtractARN = "arn:aws:lambda:${Region}:${Account}:function:${AppShortName}-${Environment}-ExtractOracle-Event-Trigger"
    lambda_invoke(source_name, OracleExtractARN)

if __name__ == "__main__":
    main()
